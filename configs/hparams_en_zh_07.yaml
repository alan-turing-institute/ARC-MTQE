# This is a config file containing experiment runs to be used for hyperparameter searching

# WandB names
wandb:
  project: MTQE
  entity: turing-arc

# The experiments defined in this file will be run once for each of the following seeds
seeds:
  - 42
  - 89
  - 107

# Any params for the slurm file that will be the same across all experiments
slurm:
  account: vjgo8416-mt-qual-est

# These are model parameters that are fixed for each experiment
hparams:
  activations: Tanh
  class_identifier: ced_model
  encoder_model: XLM-RoBERTa
  exclude_outliers: 250
  hidden_sizes:
  - 3072
  - 1024
  input_segments:
  - mt
  - src
  layer: mix
  layer_norm: false
  layer_transformation: sparsemax
  layerwise_decay: 0.95
  loss_lambda: 0.65
  optimizer: AdamW
  oversample_minority: True
  pool: avg
  pretrained_model: microsoft/infoxlm-large
  sent_layer: mix
  word_layer: 24
  word_level_training: false

# This is trainer config that is fixed for all experiments
trainer_config:
  deterministic: True
  devices: 1
  max_epochs: 100
  accumulate_grad_batches: 4
  reload_dataloaders_every_n_epochs: 1

# These are the data for each experiment
experiments:
  unfzn_lr_1e05_oversample:
    name: unfzn_lr_1e05_oversample
    train_data:
      train_1:
        dataset_name: ced
        language_pairs:
          - en-zh
    dev_data:
      dev_1:
        dataset_name: ced
        language_pairs:
          - en-zh
    hparams:
      dropout: 0.1
      learning_rate: 1.0e-05
      encoder_learning_rate: 1.0e-06
      loss: binary_cross_entropy_with_logits
      batch_size: 64
      keep_embeddings_frozen: false
      nr_frozen_epochs: 0
    slurm:
      memory: 40G
      time: 08:00:00
  unfzn_lr_1e05_oversample_layernorm:
    name: unfzn_lr_1e05_weight2
    train_data:
      train_1:
        dataset_name: ced
        language_pairs:
          - en-zh
    dev_data:
      dev_1:
        dataset_name: ced
        language_pairs:
          - en-zh
    hparams:
      dropout: 0.1
      learning_rate: 1.0e-05
      encoder_learning_rate: 1.0e-06
      loss: binary_cross_entropy_with_logits
      batch_size: 64
      keep_embeddings_frozen: false
      layer_norm: true
      nr_frozen_epochs: 0
    slurm:
      memory: 40G
      time: 08:00:00
  unfzn_lr_1e05_oversample_layernorm_calc:
    name: unfzn_lr_1e05_weight2
    train_data:
      train_1:
        dataset_name: ced
        language_pairs:
          - en-zh
    dev_data:
      dev_1:
        dataset_name: ced
        language_pairs:
          - en-zh
    hparams:
      dropout: 0.1
      learning_rate: 1.0e-05
      encoder_learning_rate: 1.0e-06
      loss: binary_cross_entropy_with_logits
      batch_size: 64
      keep_embeddings_frozen: false
      layer_norm: true
      nr_frozen_epochs: 0
      calc_threshold: true
      train_subset_size: 7000
      train_subset_replace: false
    slurm:
      memory: 40G
      time: 08:00:00
