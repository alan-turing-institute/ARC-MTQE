# This is a config file containing experiment runs to be used for hyperparameter searching

# WandB names
wandb:
  project: MTQE
  entity: turing-arc

# The experiments defined in this file will be run once for each of the following seeds
seeds:
  - 42
  - 89
  - 107

# Any params for the slurm file that will be the same across all experiments
slurm:
  account: vjgo8416-mt-qual-est

# These are model parameters that are fixed for each experiment
hparams:
  activations: Tanh
  class_identifier: ced_model
  encoder_model: XLM-RoBERTa
  exclude_outliers: 250
  hidden_sizes:
  - 3072
  - 1024
  input_segments:
  - mt
  - src
  layer: mix
  layer_norm: false
  layer_transformation: sparsemax
  layerwise_decay: 0.95
  loss_lambda: 0.65
  nr_frozen_epochs: 0
  optimizer: AdamW
  pool: avg
  pretrained_model: microsoft/infoxlm-large
  sent_layer: mix
  word_layer: 24
  word_level_training: false

# This is trainer config that is fixed for all experiments
trainer_config:
  deterministic: True
  devices: 1
  max_epochs: 100
  accumulate_grad_batches: 4

# These are the data for each experiment
experiments:
  unfzn_lr_1e05_drp0-2:
    name: unfzn_lr_1e05_drp0-2
    train_data:
      train_1:
        dataset_name: ced
        language_pairs:
          - en-ja
    dev_data:
      dev_1:
        dataset_name: ced
        language_pairs:
          - en-ja
    hparams:
      dropout: 0.2
      learning_rate: 1.0e-05
      encoder_learning_rate: 1.0e-06
      loss: binary_cross_entropy_with_logits
      batch_size: 32
      keep_embeddings_frozen: false
    slurm:
      memory: 40G
      time: 04:00:00
  unfzn_lr_1-5e05:
    name: unfzn_lr_1-5e05
    train_data:
      train_1:
        dataset_name: ced
        language_pairs:
          - en-ja
    dev_data:
      dev_1:
        dataset_name: ced
        language_pairs:
          - en-ja
    hparams:
      dropout: 0.1
      learning_rate: 1.5e-05
      encoder_learning_rate: 1.0e-06
      loss: binary_cross_entropy_with_logits
      batch_size: 32
      keep_embeddings_frozen: false
    slurm:
      memory: 40G
      time: 04:00:00
  unfzn_lr_1-5e04:
    name: unfzn_lr_1-5e04
    train_data:
      train_1:
        dataset_name: ced
        language_pairs:
          - en-ja
    dev_data:
      dev_1:
        dataset_name: ced
        language_pairs:
          - en-ja
    hparams:
      dropout: 0.1
      learning_rate: 1.5e-04
      encoder_learning_rate: 1.0e-05
      loss: binary_cross_entropy_with_logits
      batch_size: 32
      keep_embeddings_frozen: false
    slurm:
      memory: 40G
      time: 04:00:00
