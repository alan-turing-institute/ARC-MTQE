# WandB names
wandb:
  project: MTQE
  entity: turing-arc

# The experiments defined in this file will be run once for each of the following seeds
seeds:
  - 12
  - 77
  - 89

# These are model parameters that are fixed for each experiment
hparams:
  activations: Tanh
  batch_size: 16
  class_identifier: ced_model
  dropout: 0.1
  encoder_learning_rate: 1.0e-06
  encoder_model: XLM-RoBERTa
  final_activation: sigmoid
  hidden_sizes:
  - 3072
  - 1024
  input_segments:
  - mt
  - src
  layer: mix
  layer_norm: false
  layer_transformation: sparsemax
  layerwise_decay: 0.95
  learning_rate: 1.5e-05
  loss: cross-entropy
  loss_lambda: 0.65
  nr_frozen_epochs: 0.3
  optimizer: AdamW
  pool: avg
  pretrained_model: microsoft/infoxlm-large
  sent_layer: mix
  word_layer: 24
  word_level_training: false

# This is trainer config that is fixed for each experiment
trainer_config:
  deterministic: True
  devices: 1
  max_epochs: 100

# Config for early stopping callback, fixed for each experiment
early_stopping:
  monitor: val_recall
  patience: 3
  strict: True
  verbose: False
  mode: max
  min_delta: 0.

# These are the data for each experiment
experiments:
  en-cs_frozen_100:
    name: en-cs_frozen_100
    train_data:
      train_1:
        dataset_name: ced
        language_pairs:
          - en-cs
    dev_data:
      dev_1:
        dataset_name: ced
        language_pairs:
          - en-cs
    hparams:
      keep_embeddings_frozen: true

en-cs_unfrozen:
    name: en-cs_unfrozen
    train_data:
      train_1:
        dataset_name: ced
        language_pairs:
          - en-cs
    dev_data:
      dev_1:
        dataset_name: ced
        language_pairs:
          - en-cs
    hparams:
      keep_embeddings_frozen: false
