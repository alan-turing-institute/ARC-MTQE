seeds:
  - 12
  - 77
  - 89

hparams:
  activations: Tanh
  batch_size: 16
  class_identifier: ced_model
  dropout: 0.1
  encoder_learning_rate: 1.0e-06
  encoder_model: XLM-RoBERTa
  final_activation: null
  hidden_sizes:
  - 3072
  - 1024
  input_segments:
  - mt
  - src
  layer: mix
  layer_norm: false
  layer_transformation: sparsemax
  layerwise_decay: 0.95
  learning_rate: 1.5e-05
  loss: cross-entropy
  loss_lambda: 0.65
  nr_frozen_epochs: 0.3
  optimizer: AdamW
  pool: avg
  pretrained_model: microsoft/infoxlm-large
  sent_layer: mix
  word_layer: 24
  word_level_training: false

trainer_config:
  accelerator: cpu
  deterministic: True

experiments:
  en-cs_frozen:
    name: en-cs_frozen
    hparams:
      train_data:
        train_1:
          dataset_name: ced
          language_pairs:
            - en-cs
      validation_data:
        valid_1:
          dataset_name: ced
          language_pairs:
            - en-cs
      keep_embeddings_frozen: true
    trainer_config:
      max_epochs: 3

en-cs_unfrozen:
    name: en-cs_unfrozen
    hparams:
      train_data:
        train_1:
          dataset_name: ced
          language_pairs:
            - en-cs
      validation_data:
        valid_1:
          dataset_name: ced
          language_pairs:
            - en-cs
      keep_embeddings_frozen: false
    trainer_config:
      max_epochs: 3
