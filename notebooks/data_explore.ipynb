{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the ARC-MTQE directory\n",
    "main_dir = os.path.dirname(os.getcwd())\n",
    "data_dir = os.path.join(main_dir, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_stats(n_rows, n_segments, n_critical_errors):\n",
    "    \"\"\"\n",
    "    Print data summary statistics.\n",
    "    \"\"\"\n",
    "    print(f\"Number of rows: {n_rows}\")\n",
    "    print(f\"Number of segments: {n_segments}\")\n",
    "    print(f\"Number of critical errors: {n_critical_errors}\")\n",
    "    print(f\"Percentage critical errors: {n_critical_errors/n_segments}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WMT 2021 critical errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encs\n",
      "Number of rows: 9476\n",
      "Number of segments: 9476\n",
      "Number of critical errors: 1637\n",
      "Percentage critical errors: 0.17275221612494723\n",
      "\n",
      "\n",
      "ende\n",
      "Number of rows: 9878\n",
      "Number of segments: 9878\n",
      "Number of critical errors: 2773\n",
      "Percentage critical errors: 0.2807248430856449\n",
      "\n",
      "\n",
      "enja\n",
      "Number of rows: 9658\n",
      "Number of segments: 9658\n",
      "Number of critical errors: 897\n",
      "Percentage critical errors: 0.0928763719196521\n",
      "\n",
      "\n",
      "enzh\n",
      "Number of rows: 8859\n",
      "Number of segments: 8859\n",
      "Number of critical errors: 1409\n",
      "Percentage critical errors: 0.1590472965345976\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# contains train, dev and test data\n",
    "mlqe_pe_data_dir = os.path.join(data_dir, \"mlqe-pe\", \"data\")\n",
    "\n",
    "language_pairs = [\"encs\", \"ende\", \"enja\", \"enzh\"]\n",
    "\n",
    "for lp in language_pairs:\n",
    "    n_rows = 0\n",
    "    n_segments = 0\n",
    "    n_critical_errors = 0\n",
    "\n",
    "    path_dev = os.path.join(mlqe_pe_data_dir, \"catastrophic_errors\", f\"{lp}_majority_dev.tsv\")\n",
    "    path_train = os.path.join(mlqe_pe_data_dir, \"catastrophic_errors\", f\"{lp}_majority_train.tsv\")\n",
    "    path_test = os.path.join(mlqe_pe_data_dir, \"catastrophic_errors\", f\"{lp}_majority_test_blind.tsv\")\n",
    "    path_goldlabels = os.path.join(mlqe_pe_data_dir, \"catastrophic_errors_goldlabels\", f\"{lp}_majority_test_goldlabels\", \"goldlabels.txt\")\n",
    "\n",
    "    df_dev = pd.read_csv(path_dev, sep=\"\\t\", header=None, names=[\"idx\", \"source\", \"target\", \"annotations\", \"label\"])\n",
    "    df_train = pd.read_csv(path_train, sep=\"\\t\", header=None, names=[\"idx\", \"source\", \"target\", \"annotations\", \"label\"])\n",
    "    df_test = pd.read_csv(path_test, sep=\"\\t\", header=None, names=[\"idx\", \"source\", \"target\"])\n",
    "    df_labels = pd.read_csv( path_goldlabels, sep=\"\\t\", header=None, names=[\"lang_pair\", \"ref\", \"idx\", \"label\"])\n",
    "    df_test_labelled = pd.merge(df_test, df_labels, on='idx')\n",
    "\n",
    "    for df in [df_train, df_dev, df_test_labelled]:\n",
    "        n_rows += df.shape[0]\n",
    "        n_segments += df[\"idx\"].nunique()\n",
    "        n_critical_errors += df[df[\"label\"] == \"ERR\"].shape[0]\n",
    "\n",
    "    print(lp)\n",
    "    print_data_stats(n_rows, n_segments, n_critical_errors)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WMT 2022 critical errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en-de\n",
      "Number of rows: 173291\n",
      "Number of segments: 173291\n",
      "Number of critical errors: 9951\n",
      "Percentage critical errors: 0.057423640004385684\n",
      "\n",
      "\n",
      "pt-en\n",
      "Number of rows: 44862\n",
      "Number of segments: 44862\n",
      "Number of critical errors: 2593\n",
      "Percentage critical errors: 0.057799473942311975\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wmt_data_dir = os.path.join(data_dir, \"wmt-qe-2022-data\")\n",
    "\n",
    "language_pairs = [\"en-de\", \"pt-en\"]\n",
    "for lp in language_pairs:\n",
    "    n_rows = 0\n",
    "    n_segments = 0\n",
    "    n_critical_errors = 0\n",
    "\n",
    "    path_train = os.path.join(wmt_data_dir, \"train-dev_data\", \"task3_ced\", \"train\", lp, f\"{lp}-train\", \"train.label\")\n",
    "    path_dev = os.path.join(wmt_data_dir, \"train-dev_data\", \"task3_ced\", \"dev\", lp, f\"{lp}-dev\", \"dev.label\")\n",
    "    path_test = os.path.join(wmt_data_dir, \"test_data-gold_labels\", \"task3_ced\", lp, f\"test.2022.{lp}.label\")\n",
    "\n",
    "    df_train_labels = pd.read_csv(path_train, names=[\"label\"])\n",
    "    df_dev_labels = pd.read_csv(path_dev, names=[\"label\"])\n",
    "    df_test_labels = pd.read_csv(path_test, names=[\"label\"])\n",
    "\n",
    "    for df in [df_train_labels, df_dev_labels, df_test_labels]:\n",
    "        n = df.shape[0]\n",
    "        n_bad = df[df[\"label\"]==\"BAD\"].shape[0]\n",
    "\n",
    "        n_rows += n\n",
    "        n_segments += n\n",
    "        n_critical_errors += n_bad\n",
    "\n",
    "    print(lp)\n",
    "    print_data_stats(n_rows, n_segments, n_critical_errors)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMETR\n",
    "\n",
    "The below numbers do not correspond to the paper which lists 10 critical error categories. However, the dataset has 12 critical error categories + one of the baselines is listed as a critical error as well. This brings the total to 13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "demetr_data_dir = os.path.join(data_dir, \"demetr\", \"dataset\")\n",
    "dfs = []\n",
    "for filename in os.listdir(demetr_data_dir):\n",
    "    f = os.path.join(demetr_data_dir, filename)\n",
    "    df = pd.read_json(f)\n",
    "    dfs.append(df)\n",
    "\n",
    "demetr_df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of segments per language pair:  [3500]\n",
      "Number of unique segments per language pair:  [100]\n",
      "Number of error IDs per language pair:  [33]\n",
      "Number of error names per language pair:  [35]\n",
      "Number of language pairs per error category:  [10]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of segments per language pair: \", demetr_df.groupby('lang_tag')['id'].count().unique())\n",
    "print(\"Number of unique segments per language pair: \", demetr_df.groupby('lang_tag')['id'].nunique().unique())\n",
    "print(\"Number of error IDs per language pair: \", demetr_df.groupby('lang_tag')['pert_id'].nunique().unique())\n",
    "print(\"Number of error names per language pair: \", demetr_df.groupby('lang_tag')['pert_name'].nunique().unique())\n",
    "print(\"Number of language pairs per error category: \", demetr_df.groupby('pert_name')['lang_tag'].nunique().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cells show that `pert_id` values 5 and 6 are missing. Looking at the corresponding files `major_id5_pp_removed` and `critical_id6_addition.json`, the listed `pert_id` within the files is 8 instead of the expected 5 and 6. However, it seems that the `pert_name` column is used correctly in all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5, 6}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(np.arange(1, 36, 1)) - set(demetr_df['pert_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 35000\n",
      "Number of segments: 35000\n",
      "Number of critical errors: 10000\n",
      "Percentage critical errors: 0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "n_rows = n_segments = demetr_df.shape[0]\n",
    "# n_critical_errors = demetr_df[demetr_df['severity'] == 'critical'].shape[0]\n",
    "\n",
    "# note: in the paper, there are 10 critical error categories --> 10,000 segments \n",
    "n_critical_errors = 10000\n",
    "\n",
    "print_data_stats(n_rows, n_segments, n_critical_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbabel MQM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3h/4b2rm8kn7l9d6j583_k1mx1w0000gr/T/ipykernel_66044/373853115.py:3: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(file_path, sep=\"\\t\", index_col=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seg_id</th>\n",
       "      <th>category</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33976</th>\n",
       "      <td>1553</td>\n",
       "      <td>omission</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33977</th>\n",
       "      <td>1554</td>\n",
       "      <td>No-error</td>\n",
       "      <td>No-error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33978</th>\n",
       "      <td>1555</td>\n",
       "      <td>No-error</td>\n",
       "      <td>No-error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33979</th>\n",
       "      <td>1556</td>\n",
       "      <td>mistranslation</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33980</th>\n",
       "      <td>1557</td>\n",
       "      <td>mistranslation</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       seg_id        category  severity\n",
       "33976    1553        omission     major\n",
       "33977    1554        No-error  No-error\n",
       "33978    1555        No-error  No-error\n",
       "33979    1556  mistranslation     minor\n",
       "33980    1557  mistranslation     major"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = os.path.join(data_dir, \"unbabel\", \"mqm_generalMT2022_enru.tsv\")\n",
    "\n",
    "df = pd.read_csv(file_path, sep=\"\\t\", index_col=False)\n",
    "df[['seg_id', 'category', 'severity']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['crit_error'] = np.where(df[\"severity\"] == \"critical\", 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 33981\n",
      "Number of segments: 1215\n",
      "Number of critical errors: 92\n",
      "Percentage critical errors: 0.0757201646090535\n"
     ]
    }
   ],
   "source": [
    "n_rows = df.shape[0]\n",
    "n_segments = df['seg_id'].nunique()\n",
    "n_critical_errors = sum(df.groupby('seg_id')['crit_error'].sum() >= 1)\n",
    "\n",
    "print_data_stats(n_rows, n_segments, n_critical_errors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
