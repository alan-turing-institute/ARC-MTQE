{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torchmetrics import MatthewsCorrCoef\n",
    "\n",
    "from mtqe.data.loaders import load_ced_test_data\n",
    "from mtqe.utils.language_pairs import LI_LANGUAGE_PAIRS_WMT_21_CED\n",
    "from mtqe.utils.metrics import williams_test\n",
    "from mtqe.utils.paths import PREDICTIONS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions(lp, model, seed):\n",
    "    \"\"\"\n",
    "    Load predictions for language pair using model and seed.\n",
    "    NOTE: seed can be 'llm' or other string contained at the start\n",
    "    of the file name for models that were not trained.\n",
    "    \"\"\"\n",
    "    \n",
    "    if \"monolingual\" in model or \"second_step\" in model:\n",
    "        if lp == 'en-ja':\n",
    "            dir_path = os.path.join(PREDICTIONS_DIR, \"ced_data\", f\"{model}_enja\")\n",
    "        else:\n",
    "            dir_path = os.path.join(PREDICTIONS_DIR, \"ced_data\", model)\n",
    "    else:\n",
    "        dir_path = os.path.join(PREDICTIONS_DIR, \"ced_data\", model)\n",
    "    \n",
    "    # the seed is contained at the start of the filename - make sure to ignore final timestamp!\n",
    "    pred_file = [f for f in os.listdir(dir_path) if (lp in f and str(seed) in f[:20] and 'test' in f and 'csv' in f)][0]\n",
    "\n",
    "    return pd.read_csv(os.path.join(dir_path, pred_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "mcc = MatthewsCorrCoef(task=\"binary\", num_classes=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_median_seed(lp, model, all_seeds = [42, 89, 107, 928, 2710]):\n",
    "    \"\"\"\n",
    "    Find seed of model with median performance on language pair.\n",
    "    \"\"\"\n",
    "\n",
    "    true_labels = load_ced_test_data(lp)['score']\n",
    "    true_labels = torch.Tensor(true_labels)\n",
    "    mcc_by_seed = []\n",
    "    for seed in all_seeds:\n",
    "        scores = load_predictions(lp, model, seed)['score']\n",
    "        preds = torch.Tensor(scores) > 0.5\n",
    "        preds = preds.long()\n",
    "        mcc_by_seed.append(mcc(true_labels, preds).item())\n",
    "    df = pd.DataFrame({\"seeds\": all_seeds, \"MCCs\": mcc_by_seed})\n",
    "    \n",
    "    return df.loc[df['MCCs']==df['MCCs'].median()]['seeds'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS IS THE MODEL WE WANT TO COMPARE AGAINST\n",
    "\n",
    "# BASELINE_MODEL = 'baseline'\n",
    "BASELINE_MODEL = 'train_multilingual_auth_data_all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_monolingual_auth_data\n",
      "{'en-cs': [0.459, False], 'en-de': [0.478, False], 'en-ja': [0.173, True], 'en-zh': [0.28, True]}\n",
      "second_step_base_auth_data\n",
      "{'en-cs': [0.489, False], 'en-de': [0.472, False], 'en-ja': [0.243, False], 'en-zh': [0.304, False]}\n",
      "second_step_base_demetr_data\n",
      "{'en-cs': [0.489, False], 'en-de': [0.484, False], 'en-ja': [0.255, False], 'en-zh': [0.27, True]}\n",
      "second_step_base_demetr_auth_data\n",
      "{'en-cs': [0.472, False], 'en-de': [0.503, False], 'en-ja': [0.137, True], 'en-zh': [0.244, True]}\n",
      "prompt_basic\n",
      "{'en-cs': [0.39, True], 'en-de': [0.368, True], 'en-ja': [0.239, False], 'en-zh': [0.327, False]}\n",
      "prompt_GEMBA\n",
      "{'en-cs': [0.387, True], 'en-de': [0.333, True], 'en-ja': [0.193, True], 'en-zh': [0.308, False]}\n",
      "wmt21_annotator\n",
      "{'en-cs': [0.422, True], 'en-de': [0.475, False], 'en-ja': [0.187, True], 'en-zh': [0.294, False]}\n"
     ]
    }
   ],
   "source": [
    "all_seeds = [42, 89, 107, 928, 2710]\n",
    "\n",
    "if 'baseline' not in BASELINE_MODEL:\n",
    "    baseline_median_seeds = {}\n",
    "    for lp in LI_LANGUAGE_PAIRS_WMT_21_CED:\n",
    "\n",
    "        baseline_median_seeds[lp] = find_median_seed(lp, BASELINE_MODEL)\n",
    "\n",
    "for EXPERIMENT_GROUP_NAME in ['train_monolingual_auth_data', 'train_multilingual_auth_data_all', 'second_step_base_auth_data', 'second_step_base_demetr_data', 'second_step_base_demetr_auth_data', 'prompt_basic', 'prompt_GEMBA', 'wmt21_annotator']:\n",
    "    if EXPERIMENT_GROUP_NAME != BASELINE_MODEL:\n",
    "        print(EXPERIMENT_GROUP_NAME)\n",
    "        exp_results = {}\n",
    "        for lp in LI_LANGUAGE_PAIRS_WMT_21_CED:\n",
    "            labels_df = load_ced_test_data(lp)\n",
    "\n",
    "            if 'prompt' not in EXPERIMENT_GROUP_NAME and 'wmt21' not in EXPERIMENT_GROUP_NAME:\n",
    "                exp_median_seed = find_median_seed(lp, EXPERIMENT_GROUP_NAME)\n",
    "            else:\n",
    "                exp_median_seed = 'llm'\n",
    "            exp_df = load_predictions(lp, EXPERIMENT_GROUP_NAME, exp_median_seed)\n",
    "\n",
    "            if 'baseline' not in BASELINE_MODEL:\n",
    "                baseline_df = load_predictions(lp, BASELINE_MODEL, baseline_median_seeds[lp])\n",
    "            else:\n",
    "                baseline_df = pd.read_csv(os.path.join(PREDICTIONS_DIR, \"ced_data\", \"baseline\", f\"{lp}_test_baseline_cometkiwi_22.csv\"))\n",
    "                    \n",
    "            merged_df = baseline_df.merge(exp_df, on=\"idx\")\n",
    "            full_df = merged_df.merge(labels_df, on=\"idx\")\n",
    "\n",
    "            true_labels = torch.Tensor(full_df['score'])\n",
    "\n",
    "            baseline_preds = torch.Tensor(full_df['score_x']) > 0.5\n",
    "            baseline_preds = baseline_preds.long()\n",
    "            \n",
    "            exp_preds = torch.Tensor(full_df['score_y']) > 0.5\n",
    "            exp_preds = exp_preds.long()\n",
    "\n",
    "            baseline_mcc = mcc(true_labels, baseline_preds)\n",
    "            exp_mcc = mcc(true_labels, exp_preds)\n",
    "            metrics_mcc = mcc(baseline_preds, exp_preds)\n",
    "\n",
    "            exp_results[lp] = [round(exp_mcc.item(), 3), williams_test(baseline_mcc, exp_mcc, metrics_mcc) < 0.05]\n",
    "\n",
    "        print(exp_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
