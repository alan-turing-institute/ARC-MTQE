{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook that analyses annotator agreement - still need to tidy up and add some comments to this notebook, or to delete it as I'm not sure we will make use of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from mtqe.data.loaders import load_ced_test_data, load_ced_data\n",
    "from mtqe.utils.paths import EVAL_DIR, PREDICTIONS_DIR\n",
    "from mtqe.utils.plots import create_annotator_cm_plot\n",
    "from mtqe.utils.language_pairs import LI_LANGUAGE_PAIRS_WMT_21_CED\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.calibration import calibration_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_GROUP_NAME = 'train_monolingual_auth_data'\n",
    "DATA_SPLIT = 'dev'\n",
    "THRESHOLD_STRATEGY = 'default'\n",
    "FILE_SUFFIX = 'median_results.csv'\n",
    "CHART_NAME = 'Monolingual Authentic Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_files = [os.path.join(EVAL_DIR, EXPERIMENT_GROUP_NAME, file) for file in os.listdir(os.path.join(EVAL_DIR, EXPERIMENT_GROUP_NAME)) if file.endswith(FILE_SUFFIX)]\n",
    "enja_separate = False\n",
    "if os.path.isdir(os.path.join(EVAL_DIR, EXPERIMENT_GROUP_NAME + '_enja')):\n",
    "    eval_files.extend([os.path.join(EVAL_DIR, EXPERIMENT_GROUP_NAME + '_enja', file) for file in os.listdir(os.path.join(EVAL_DIR, EXPERIMENT_GROUP_NAME + '_enja')) if file.endswith(FILE_SUFFIX)])\n",
    "    enja_separate = True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for file in eval_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "df_meta_results = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta_results = df_meta_results[(df_meta_results['threshold_strategy'] == THRESHOLD_STRATEGY) & (df_meta_results['split'] == DATA_SPLIT)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lists of data to create plot\n",
    "preds = []\n",
    "targets = []\n",
    "indices = []\n",
    "titles = []\n",
    "\n",
    "for lp in LI_LANGUAGE_PAIRS_WMT_21_CED:\n",
    "    seed = df_meta_results[df_meta_results['language_pair']==lp].iloc[0]['seed']\n",
    "    mcc = df_meta_results[df_meta_results['language_pair']==lp].iloc[0]['MCC']\n",
    "    precision = df_meta_results[df_meta_results['language_pair']==lp].iloc[0]['precision']\n",
    "    recall = df_meta_results[df_meta_results['language_pair']==lp].iloc[0]['recall']\n",
    "    titles.append('Language pair:' + lp)# + '\\nMCC:' + str(mcc.round(3)) + '\\nPrecision:' + str(precision.round(3)) + '\\nRecall:' + str(recall.round(3)))\n",
    "    threshold = df_meta_results[df_meta_results['language_pair']==lp].iloc[0]['threshold']\n",
    "    file_prefix = lp + '_' + DATA_SPLIT + '_' + str(seed)\n",
    "    if lp == 'en-ja' and enja_separate:\n",
    "        folder = EXPERIMENT_GROUP_NAME + '_enja'\n",
    "    else:\n",
    "        folder = EXPERIMENT_GROUP_NAME\n",
    "    for file in os.listdir(os.path.join(PREDICTIONS_DIR, 'ced_data', folder)):\n",
    "        if file.startswith(file_prefix) and file.endswith('.csv'):\n",
    "            # found predictions\n",
    "            df_preds = pd.read_csv(os.path.join(PREDICTIONS_DIR, 'ced_data', folder, file))\n",
    "            scores = df_preds['score'].to_numpy()\n",
    "            scores = 1 - scores\n",
    "            binary_scores = scores > threshold\n",
    "            binary_scores = binary_scores.astype('int')\n",
    "            df_preds['binary_score'] = binary_scores\n",
    "            # preds.append(binary_scores)\n",
    "            preds.append(df_preds)\n",
    "    if DATA_SPLIT == 'test':\n",
    "        targets.append(1 - load_ced_test_data(lp)['score'].to_numpy())\n",
    "    else:\n",
    "        df = load_ced_data(DATA_SPLIT, lp, incl_annotations=True)\n",
    "        df['annotations'] = df['annotations'].str.replace('[', '')\n",
    "        df['annotations'] = df['annotations'].str.replace(']', '')\n",
    "        df[['annotator_1', 'annotator_2', 'annotator_3']] = df['annotations'].str.split(',', expand=True).astype('int')\n",
    "        df['total_score']= df['annotator_1'] + df['annotator_2'] + df['annotator_3']\n",
    "        # targets.append(df['total_score'])\n",
    "        targets.append(df)\n",
    "        # targets.append(df['annotations'])\n",
    "        indices.append(load_ced_data(DATA_SPLIT, lp)['idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_targets = []\n",
    "preds_cleaned = []\n",
    "targets_cleaned = []\n",
    "\n",
    "for ind, _ in enumerate(preds):\n",
    "    df = pd.concat([preds[ind], targets[ind]], axis=1)\n",
    "    print('shape before:', df.shape)\n",
    "    df = df[(df['annotator_1'].isin([0,1])) & (df['annotator_2'].isin([0,1])) & (df['annotator_3'].isin([0,1]))]\n",
    "    print('shape after: ', df.shape)\n",
    "    preds_targets.append(df)\n",
    "    preds_cleaned.append(df['binary_score'])\n",
    "    targets_cleaned.append(df['total_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = create_annotator_cm_plot(CHART_NAME + ' - ' + DATA_SPLIT + ' - Annotator scores vs predictions', LI_LANGUAGE_PAIRS_WMT_21_CED, preds_cleaned, targets_cleaned)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtqe-Yrqycps9-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
