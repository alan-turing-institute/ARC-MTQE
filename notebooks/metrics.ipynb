{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook produces plots to illustrate metrics such as MCC, precision and recall. Curently the plot it will produce are confusion matrices and precision-recall curves. The plots are made for one combination of experiment group / threshold strategy / data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from mtqe.data.loaders import load_ced_test_data\n",
    "from mtqe.utils.paths import EVAL_DIR, PREDICTIONS_DIR\n",
    "from mtqe.utils.plots import create_confusion_matrix_plot, create_precision_recall_curve\n",
    "from mtqe.utils.language_pairs import LI_LANGUAGE_PAIRS_WMT_21_CED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters to be used in the rest of the notebook\n",
    "\n",
    "EXPERIMENT_GROUP_NAME = 'wmt21_annotator'\n",
    "LLM = True\n",
    "DATA_SPLIT = 'test'\n",
    "THRESHOLD_STRATEGY = 'default'\n",
    "FILE_SUFFIX = 'median_results.csv'\n",
    "CONFUSION_MATRIX_NAME = 'Trained Model - Multilingual Authentic Data'\n",
    "PR_PLOT_NAME = 'Trained Model - Multilingual Authentic Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify folders of evaluation data for the experiment group - the enja evaluations might be in a separate folder.\n",
    "eval_files = [os.path.join(EVAL_DIR, EXPERIMENT_GROUP_NAME, file) for file in os.listdir(os.path.join(EVAL_DIR, EXPERIMENT_GROUP_NAME)) if file.endswith(FILE_SUFFIX)]\n",
    "enja_separate = False\n",
    "if os.path.isdir(os.path.join(EVAL_DIR, EXPERIMENT_GROUP_NAME + '_enja')):\n",
    "    eval_files.extend([os.path.join(EVAL_DIR, EXPERIMENT_GROUP_NAME + '_enja', file) for file in os.listdir(os.path.join(EVAL_DIR, EXPERIMENT_GROUP_NAME + '_enja')) if file.endswith(FILE_SUFFIX)])\n",
    "    enja_separate = True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the evaluation results into a dataframe\n",
    "dfs = []\n",
    "for file in eval_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "df_eval_results = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the results on threshold strategy and data split\n",
    "df_eval_results = df_eval_results[(df_eval_results['threshold_strategy'] == THRESHOLD_STRATEGY) & (df_eval_results['split'] == DATA_SPLIT)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lists of data to create plot\n",
    "# NOTE: The file_prefix needs to be changed if an LLM prompt is to be evaluated - this could be make more\n",
    "# robust in the code so that a manual change is not necessary.\n",
    "preds = []\n",
    "preds_probs = []\n",
    "targets = []\n",
    "cm_titles = [] # titles for confusion matrix plots\n",
    "pr_titles = [] # titles for precision-recall plots\n",
    "for lp in LI_LANGUAGE_PAIRS_WMT_21_CED:\n",
    "    seed = df_eval_results[df_eval_results['language_pair']==lp].iloc[0]['seed']\n",
    "    mcc = df_eval_results[df_eval_results['language_pair']==lp].iloc[0]['MCC']\n",
    "    precision = df_eval_results[df_eval_results['language_pair']==lp].iloc[0]['precision']\n",
    "    recall = df_eval_results[df_eval_results['language_pair']==lp].iloc[0]['recall']\n",
    "    cm_titles.append('Language pair:' + lp.upper() + '\\nMCC:' + str(mcc.round(3)) + '\\nPrecision:' + str(precision.round(3)) + '\\nRecall:' + str(recall.round(3)))\n",
    "    pr_titles.append('Language pair:' + lp.upper())\n",
    "    threshold = df_eval_results[df_eval_results['language_pair']==lp].iloc[0]['threshold']\n",
    "    if LLM:\n",
    "        file_prefix = lp + '_' + DATA_SPLIT + '_llm' \n",
    "    else:\n",
    "        file_prefix = lp + '_' + DATA_SPLIT + '_' + str(seed) \n",
    "    if lp == 'en-ja' and enja_separate:\n",
    "        folder = EXPERIMENT_GROUP_NAME + '_enja'\n",
    "    else:\n",
    "        folder = EXPERIMENT_GROUP_NAME\n",
    "    for file in os.listdir(os.path.join(PREDICTIONS_DIR, 'ced_data', folder)):\n",
    "        if file.startswith(file_prefix) and file.endswith('.csv'):\n",
    "            # found predictions\n",
    "            df_preds = pd.read_csv(os.path.join(PREDICTIONS_DIR, 'ced_data', folder, file))\n",
    "            scores = df_preds['score'].to_numpy()\n",
    "            scores = 1 - scores\n",
    "            preds_probs.append(scores)\n",
    "            binary_scores = scores > threshold\n",
    "            binary_scores = binary_scores.astype('int')\n",
    "            preds.append(binary_scores)\n",
    "    targets.append(1 - load_ced_test_data(lp)['score'].to_numpy())\n",
    "\n",
    "print(len(preds))\n",
    "print(len(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = create_confusion_matrix_plot(CONFUSION_MATRIX_NAME, cm_titles, preds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = create_precision_recall_curve(PR_PLOT_NAME, pr_titles, preds_probs, targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtqe-Yrqycps9-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
